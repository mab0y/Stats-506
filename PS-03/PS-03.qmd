---
title: "PS #03"
author: Boyuan Ma
github: https://github.com/mab0y/Stats-506/tree/main/PS-03
format: 
  html:
    embed-resources: true
toc: true
---
[Link to GitHub](https://github.com/mab0y/Stats-506/tree/main/PS-03) 

I have attached all relevant resources and links that helped me complete this problem in the form of hyperlinks.

## Problem 1 - Vision
### a.
Import `DEMO_D.XPT` and save it to `DEMO_D.dta` file:
```stata
. cd C:\Users\mab0y\OneDrive\文档\Stats-506\PS-03
C:\Users\mab0y\OneDrive\文档\Stats-506\PS-03

. import sasxport5 DEMO_D.XPT

. save DEMO_D
file DEMO_D.dta saved
```

Import `VIX_D.XPT` and [merge](https://www.stata.com/manuals/dmerge.pdf) it with `DEMO_D`, create a new database called `MERGED`:
```stata
. import sasxport5 VIX_D.XPT

. merge 1:1 seqn using DEMO_D, keep(match)

    Result                      Number of obs
    -----------------------------------------
    Not matched                             0
    Matched                             6,980  (_merge==3)
    -----------------------------------------

. save MERGED
file MERGED.dta saved

. count
  6,980
```

### b.
Generate a new variable `viq220_1` which drop `Don't know` and `Missing` values and convert `viq220` to dummy variables:
```stata
. gen viq220_1 = .
(6,980 missing values generated)

. replace viq220_1 = 0 if viq220 == 2
(3,780 real changes made)

. replace viq220_1 = 1 if viq220 == 1
(2,765 real changes made)
```

Create classification on age based on respondents age. Here I used [floor function](https://www.statalist.org/forums/forum/general-stata-discussion/general/1367450-rounding-down-every-output-to-integral-numbers
) to round down to the nearest multiple of ten combining with a [loop](https://stackoverflow.com/questions/26857269/how-to-change-string-value-label) to create this classification:
```stata
. gen ridageyr_1 = floor(ridageyr/10)*10

. gen ridageyr_1_class = "Not Available"

. foreach i of numlist 0/9 {
  2.         local start = 10*`i'
  3.         local end = 10*`i'+9
  4.         replace ridageyr_1_class = "`start'-`end'" if ridageyr_1 == `i'*10
  5. }
(0 real changes made)
(2,207 real changes made)
(1,021 real changes made)
(818 real changes made)
(815 real changes made)
(631 real changes made)
(661 real changes made)
(469 real changes made)
(358 real changes made)
(0 real changes made)
```
Use collapse to create the table:
```stata
. preserve

. collapse (mean) viq220_1, by(ridageyr_1_class)

. list

     +---------------------+
     | ridage~s   viq220_1 |
     |---------------------|
  1. |    10-19   .3208812 |
  2. |    20-29   .3265742 |
  3. |    30-39   .3586667 |
  4. |    40-49   .3699871 |
  5. |    50-59   .5500821 |
     |---------------------|
  6. |    60-69   .6222222 |
  7. |    70-79   .6689038 |
  8. |    80-89   .6688103 |
     +---------------------+

. restore
```
### c.
Generate dummy variables based on `ridreth1`:
```stata
. gen mexican_american_dummy = (ridreth1 == 1)

. gen other_hispanic_dummy = (ridreth1 == 2)

. gen non_hispanic_white_dummy = (ridreth1 == 3)

. gen non_hispanic_black_dummy = (ridreth1 == 4)
```
Generate a new variable `riagendr_1` to convert `riagendr` to dummy variables and add label `female` and `male` to it:
```stata
. gen riagendr_1 = (riagendr==1)

. label define sex 0 "female" 1 "male"

. label values riagendr_1 sex
```
Fit the three logistic logistic regression:
```stata
. logit viq220_1 ridageyr, or

Iteration 0:   log likelihood = -4457.6265  
Iteration 1:   log likelihood = -4236.2351  
Iteration 2:   log likelihood = -4235.9433  
Iteration 3:   log likelihood = -4235.9433  

Logistic regression                                     Number of obs =  6,545
                                                        LR chi2(1)    = 443.37
                                                        Prob > chi2   = 0.0000
Log likelihood = -4235.9433                             Pseudo R2     = 0.0497

------------------------------------------------------------------------------
    viq220_1 | Odds ratio   Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
    ridageyr |    1.02498   .0012356    20.47   0.000     1.022561    1.027405
       _cons |    .283379   .0151461   -23.59   0.000     .2551952    .3146755
------------------------------------------------------------------------------
Note: _cons estimates baseline odds.

. estimates store model1

. 
. logit viq220_1 ridageyr mexican_american_dummy other_hispanic_dummy non_hispanic_white_dummy non_h
> ispanic_black_dummy riagendr_1 , or

Iteration 0:   log likelihood = -4457.6265  
Iteration 1:   log likelihood = -4138.3859  
Iteration 2:   log likelihood = -4136.8807  
Iteration 3:   log likelihood = -4136.8805  

Logistic regression                                     Number of obs =  6,545
                                                        LR chi2(6)    = 641.49
                                                        Prob > chi2   = 0.0000
Log likelihood = -4136.8805                             Pseudo R2     = 0.0720

------------------------------------------------------------------------------------------
                viq220_1 | Odds ratio   Std. err.      z    P>|z|     [95% conf. interval]
-------------------------+----------------------------------------------------------------
                ridageyr |   1.022831   .0012912    17.88   0.000     1.020303    1.025365
  mexican_american_dummy |   .5215282   .0706186    -4.81   0.000     .3999623    .6800432
    other_hispanic_dummy |   .6097722   .1203815    -2.51   0.012     .4141169    .8978675
non_hispanic_white_dummy |   1.018101   .1324116     0.14   0.890     .7890156    1.313699
non_hispanic_black_dummy |   .6776529    .090558    -2.91   0.004     .5215034     .880557
              riagendr_1 |   .6052646   .0320857    -9.47   0.000     .5455346    .6715345
                   _cons |   .5048045   .0661152    -5.22   0.000     .3905168    .6525393
------------------------------------------------------------------------------------------
Note: _cons estimates baseline odds.

. estimates store model2

. 
. logit viq220_1 ridageyr mexican_american_dummy other_hispanic_dummy non_hispanic_white_dummy non_h
> ispanic_black_dummy riagendr_1 indfmpir, or

Iteration 0:   log likelihood = -4259.5533  
Iteration 1:   log likelihood = -3948.3256  
Iteration 2:   log likelihood = -3946.9043  
Iteration 3:   log likelihood = -3946.9041  

Logistic regression                                     Number of obs =  6,247
                                                        LR chi2(7)    = 625.30
                                                        Prob > chi2   = 0.0000
Log likelihood = -3946.9041                             Pseudo R2     = 0.0734

------------------------------------------------------------------------------------------
                viq220_1 | Odds ratio   Std. err.      z    P>|z|     [95% conf. interval]
-------------------------+----------------------------------------------------------------
                ridageyr |   1.022436    .001324    17.14   0.000     1.019845    1.025035
  mexican_american_dummy |    .587002   .0822693    -3.80   0.000     .4460076    .7725683
    other_hispanic_dummy |   .6592157   .1333587    -2.06   0.039     .4434353     .979997
non_hispanic_white_dummy |   .9692835   .1294291    -0.23   0.815     .7460866    1.259251
non_hispanic_black_dummy |   .7222799   .0991681    -2.37   0.018     .5518698    .9453104
              riagendr_1 |   .5967415    .032406    -9.51   0.000     .5364902    .6637595
                indfmpir |   1.120301   .0198376     6.42   0.000     1.082087    1.159865
                   _cons |   .3801607   .0543067    -6.77   0.000     .2873237    .5029942
------------------------------------------------------------------------------------------
Note: _cons estimates baseline odds.

. estimates store model3
```

This is the [summary table](https://stats.oarc.ucla.edu/stata/faq/how-can-i-use-estout-to-make-regression-tables-that-look-like-those-in-journal-articles/) presenting the estimation results:
```
. estout model1 model2 model3, cells(b(star fmt(3)) se(par fmt(2))) eform stats(N r2_p aic,fmt(0 3 3
> ))

------------------------------------------------------------
                   model1          model2          model3   
                     b/se            b/se            b/se   
------------------------------------------------------------
viq220_1                                                    
ridageyr            1.025***        1.023***        1.022***
                   (0.00)          (0.00)          (0.00)   
mexican_am~y                        0.522***        0.587***
                                   (0.07)          (0.08)   
other_hisp~y                        0.610*          0.659*  
                                   (0.12)          (0.13)   
non_~e_dummy                        1.018           0.969   
                                   (0.13)          (0.13)   
non_~k_dummy                        0.678**         0.722*  
                                   (0.09)          (0.10)   
riagendr_1                          0.605***        0.597***
                                   (0.03)          (0.03)   
indfmpir                                            1.120***
                                                   (0.02)   
_cons               0.283***        0.505***        0.380***
                   (0.02)          (0.07)          (0.05)   
------------------------------------------------------------
N                    6545            6545            6247   
r2_p                0.050           0.072           0.073   
aic              8475.887        8287.761        7909.808   
------------------------------------------------------------
```

### d.
In the second and third logistic regressions, the `riagendr_1` variable is significant under the 1% confidence level, so the odds of men and women being wears of glasess/contact lenses for distance vision differs.

Use [two sample z-test](https://www.ssc.wisc.edu/sscc/pubs/sfs/sfs-prtest.htm) to test whether the proportion of wearers of glasses/contact lenses for distance vision differs between men and women:
```stata
. prtest viq220_1, by(riagendr_1)

Two-sample test of proportions                female: Number of obs =     3350
                                                male: Number of obs =     3195
------------------------------------------------------------------------------
       Group |       Mean   Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
      female |   .4728358   .0086259                      .4559293    .4897423
        male |   .3696401   .0085398                      .3529023    .3863778
-------------+----------------------------------------------------------------
        diff |   .1031958   .0121382                      .0794054    .1269861
             |  under H0:   .0122146     8.45   0.000
------------------------------------------------------------------------------
        diff = prop(female) - prop(male)                          z =   8.4485
    H0: diff = 0

    Ha: diff < 0                 Ha: diff != 0                 Ha: diff > 0
 Pr(Z < z) = 1.0000         Pr(|Z| > |z|) = 0.0000          Pr(Z > z) = 0.0000
```
The difference and standard error under null hypothesis are calculated to get Z-stat. p-value = 0.0000, we should reject the null hypothesis. The proportion of wearers of glasses/contact lenses for distance vision differs between men and women.

## Problem 2 - Sakila
### a.
```{r}
#| echo: false
library(DBI) 
sakila <- dbConnect(RSQLite::SQLite(), "~/sakila_master.db")
```
All films are in English:
```{r}
dbGetQuery(sakila,"SELECT l.name, COUNT(f.film_id) FROM film f JOIN language l ON f.language_id = l.language_id GROUP BY l.name")
```

### b.
Extract `film_category` and `category`:
```{r}
film_category<-dbGetQuery(sakila,"SELECT * FROM film_category")
category<-dbGetQuery(sakila,"SELECT * FROM category")
```
`film_id` and `category_id` are one to one relationship, Just need to sort the table of each `category_id` and find the corresponding `category_name` to each `category_id`.Sports is the most common in the data and 74 movies are of this genre:
```{r}
max_category_id<-sort(table(film_category$category_id),decreasing = TRUE)
max_category_name<-cbind(category$name[as.numeric(names(max_category_id))],as.vector(max_category_id))
```
```{r}
#| echo: false
max_category_name
```
Use SQL query to get the same result:
```{r}
dbGetQuery(sakila, "SELECT c.name,COUNT(fc.film_id) as number FROM film_category fc JOIN category c ON fc.category_id = c.category_id GROUP BY c.name ORDER BY number DESC")
```

### c.

Extract `customer`, `address`, `city` and `country`:
```{r}
customer<-dbGetQuery(sakila,"SELECT * FROM customer")
address<-dbGetQuery(sakila,"SELECT * FROM address")
city<-dbGetQuery(sakila,"SELECT * FROM city")
country<-dbGetQuery(sakila,"SELECT * FROM country")
```
I use [`merge`](https://www.statmethods.net/management/merging.html) to merge these four data_frames together on their foreign keys:
```{r}
#| warning: false
cust_address<-merge(customer,address,by="address_id")
cust_address_city<-merge(cust_address,city,by="city_id")
cust_address_city_country<-merge(cust_address_city,country,by="country_id")
``` 
United Kingdom have exactly 9 customers:
```{r}
country_table<-table(cust_address_city_country$country)
country_name <- country_table[country_table == 9]
```
```{r}
#| echo: false
country_name
```
Use SQL query to get the same result:
```{r}
dbGetQuery(sakila, "SELECT country.country,COUNT(cust.customer_id) as number FROM customer cust JOIN address a ON cust.address_id=a.address_id JOIN city ON a.city_id=city.city_id JOIN country ON city.country_id=country.country_id GROUP BY country.country HAVING number=9")
```

## Problem 3 - US Records
```{r}
#| echo: false
us_500 <- read.csv("~/us-500.csv",header=TRUE)
```

### a.
14% of email addresses are hosted at a domain with TLD “.net”:
```{r}
length(grep("\\.net$", us_500$email))/length(us_500$email)
```

### b.
50.6% of email addresses have at least one non alphanumeric character in them:
```{r}
length(grep("[^a-zA-Z0-9]+[a-zA-Z0-9]*@", us_500$email))/length(us_500$email)
```

### c.
973 is the most common area code amongst all phone numbers:
```{r}
area_code<-table(sapply(us_500$phone1,substr,start=1,stop=3))
area_code_most_common<-area_code[which.max(area_code)]
```
```{r}
#| echo: false
area_code_most_common
```
### d.
[Remove](https://stackoverflow.com/questions/9704213/remove-part-of-a-string) the part of the string before the apartment numbers by replacing it with an empty string:
```{r}
address_apt<-us_500$address[grep(".+#", us_500$address)]
apt_num<-gsub(pattern=".+#", replacement = "", address_apt)
hist(log(as.numeric(apt_num)))
```
### e.
Take the leading digit and create a bar chart. It appears that the apartment numbers do not adhere to Benford's Law, suggesting that they may not represent real data:
```{r}
apt_num_leading<-as.numeric(substr(apt_num,1,1))
barplot(table(apt_num_leading))
```
The probability distribution of the real data significantly diverges from Benford's law distribution:
```{r}
apt_num_leading_dist<-table(apt_num_leading)/length(apt_num_leading)
benford<-log(1+1/c(1:9),base=10)
names(benford)<-1:9

cbind(apt_num_leading_dist,benford)
```
### f.
Split the address string by spaces, extract the first part (which is the street number), convert it to a numeric value, and then take the modulus of it by 10 to obtain the last digit:
```{r}
st_split<-strsplit(us_500$address," ")
st_num <- as.numeric(sapply(st_split, function(x) x[1]))
st_num_last<-st_num %%10
st_num_last<-st_num_last[st_num_last>0]
```
Take the last digit and create a bar chart. It appears that the apartment numbers do not adhere to Benford's Law, suggesting that they may not represent real data:
```{r}
barplot(table(st_num_last))
```
The probability distribution of the real data significantly diverges from Benford's law distribution:
```{r}
st_num_last_dist<-table(st_num_last)/length(st_num_last)
cbind(st_num_last_dist,benford)
```