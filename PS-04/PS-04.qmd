---
title: "PS #04"
author: Boyuan Ma
github: https://github.com/mab0y/Stats-506/tree/main/PS-04
format: 
  html:
    embed-resources: true
toc: true
---

[Link to GitHub](https://github.com/mab0y/Stats-506/tree/main/PS-04)

I have provided hyperlinks to all pertinent resources and references that assisted me in addressing this problem.

## Problem 1 - Tidyverse

### a.

Import the libraries and save data it to the environment:

```{r}
#| include = FALSE
library(tidyverse)
```

```{r}
library(nycflights13)

airlines_data <- airlines
airports_data <- airports
flights_data <- flights
planes_data <- planes
weather_data <- weather
```

Use [`left_join`](https://stackoverflow.com/questions/1299871/how-to-join-merge-data-frames-inner-outer-left-right) to merge `flights` and `airports` by `origin` and `faa`:

```{r}
flights_data_origin <- flights_data %>% left_join(airports_data, by = c("origin" = "faa"))
```

Group flights by airport name and calculate the mean and median departure delay per airport:

```{r}
flights_data_origin_summary <- flights_data_origin %>%
  group_by(name) %>%
  summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE), median_dep_delay = median(dep_delay, na.rm = TRUE)) %>%
  filter(!is.na(name)) %>%
  arrange(desc(mean_dep_delay))%>%
  ungroup()
```

```{r}
#| echo: false
print(flights_data_origin_summary,n=Inf)
```

Repeat the steps for `dest` and `faa`:

```{r}
flights_data_dest <- flights_data %>% left_join(airports_data, by = c("dest" = "faa"))
```

[Count](https://stackoverflow.com/questions/22767893/count-number-of-rows-by-group-using-dplyr) the rows of each group and exclude any destination with under 10 flights:

```{r}
flights_data_dest_summary <- flights_data_dest %>%
  group_by(name) %>%
  summarize(mean_arr_delay = mean(arr_delay, na.rm = TRUE), median_arr_delay = median(arr_delay, na.rm = TRUE), n_row = n()) %>%
  filter(n_row>=10& !is.na(name)) %>%
  arrange(desc(mean_arr_delay))%>%
  select(-n_row)%>%
  ungroup()
```

```{r}
#| echo: false
print(flights_data_dest_summary,n=Inf)
```

### b.

Merge `flights` and `planes` by `tailnum`:

```{r}
flights_data_model <- flights_data %>% left_join(planes, by = "tailnum")
```

Calculate average speed for each model:

```{r}
flights_data_model_summary <- flights_data_model %>%
  group_by(model) %>%
  summarize(average_speed=mean(distance/(air_time/60)), count=n())%>%
  filter(!is.na(model)) %>%
  arrange(desc(average_speed)) %>%
  ungroup() %>%
  head(1)
```

```{r}
#| echo: false
print(flights_data_model_summary,n=Inf)
```

## Problem 2 - get_temp()

```{r}
#| echo: false
nnmaps <- read.csv("~/chicago-nmmaps.csv")
nnmaps$date <- as.Date(nnmaps$date)
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#' get_temp()
#'
#' @param month Month, either a numeric 1-12 or a string
#' @param year A numeric year
#' @param data The data set to obtain data from
#' @param celsius Logically indicating whther the results should be in celsius. Default FALSE
#' @param average_fn  A function with which to compute the mean. Default is mean
#'
#' @return The average temperature for a given month
get_temp <- function(month, year, data, celsius=FALSE, average_fn=mean){
  month_names <- c(
    "January", "February", "March", "April", "May", "June", 
    "July", "August", "September", "October", "November", "December"
  )
  
  if (year>=1997 && year<=2000){year_num <- year}
    else{
      stop("Invalid Year")
    }

  if (is.character(month)) {
    month_num <- which(grepl(pattern = month, x = month_names, ignore.case = TRUE))
  }
    else if (is.numeric(month) && month >= 1 && month <= 12) {
    month_num <- month
    } 
      else {
        stop("Invalid Month")
      }

  result <- data %>% 
    filter(year(date) == year_num & month(date) == month_num) %>%
      summarize(avg_temp = average_fn(temp))
  
  if(celsius) {
    result <- result %>%
      mutate(avg_temp = (avg_temp - 32) * 5/9)
  }
  
  return(result$avg_temp)
}
```

```{r}
#| error: true
get_temp("Apr", 1999, data = nnmaps)
get_temp("Apr", 1999, data = nnmaps, celsius = TRUE)
get_temp(10, 1998, data = nnmaps, average_fn = median)
get_temp(13, 1998, data = nnmaps)
get_temp(2, 2005, data = nnmaps)
get_temp("November", 1999, data =nnmaps, celsius = TRUE,
         average_fn = function(x) {
           x %>% sort -> x
           x[2:(length(x) - 1)] %>% mean %>% return
         })
```

## Problem 3 - SAS

### a.
[a-results](https://github.com/mab0y/Stats-506/blob/main/PS-04/a-results.html) 

California has the highest percentage of records and 2.08956% of all records correspond to Michigan:
``` sas
%let in_path = ~/PS04_Problem3_SAS/input_data;
%let out_path = ~/PS04_Problem3_SAS/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data in_lib.recs2020_public_v5;
  set in_lib.recs2020_public_v5;
  nweight_sq = nweight * nweight;
run; 

proc summary data=in_lib.recs2020_public_v5;
  class state_name; 
  output out=out_lib.ess_by_states_recs2020
    sum(nweight) = num
    sum(nweight_sq) = den;
run; 

data out_lib.ess_by_states_recs2020; 
  set out_lib.ess_by_states_recs2020;
  where _type_ = 1; 
  ess = num * num / den; 
  drop num den _type_;
run;

proc summary data=out_lib.ess_by_states_recs2020;
  var ess;
  output out = out_lib.ess_total_sum (drop=_type_ _freq_) 
  sum=total_ess;
run;

data out_lib.ess_by_states_recs2020_pct;
  set out_lib.ess_by_states_recs2020;
  if _n_ = 1 then set out_lib.ess_total_sum;
  pct = ess / total_ess * 100;
run;

proc sort data=out_lib.ess_by_states_recs2020_pct
  out = out_lib.ess_by_states_recs2020_pct_sort;
  by descending pct;
run;

data out_lib.michigan_pct;
    set out_lib.ess_by_states_recs2020_pct;
    where state_name = "Michigan";
run;

proc print data = out_lib.ess_by_states_recs2020_pct_sort;
proc print data = out_lib.michigan_pct;
```

### b.
[b-results](https://github.com/mab0y/Stats-506/blob/main/PS-04/b-results.html) 
```sas
%let in_path = ~/PS04_Problem3_SAS/input_data;
%let out_path = ~/PS04_Problem3_SAS/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data out_lib.positive_electricity_cost;
	set in_lib.recs2020_public_v5;
	where dollarel > 0;
run;

proc univariate data=out_lib.positive_electricity_cost;
    var dollarel;
    histogram dollarel;
run;
```

### c.
[c-results](https://github.com/mab0y/Stats-506/blob/main/PS-04/c-results.html) 
```sas
%let in_path = ~/PS04_Problem3_SAS/input_data;
%let out_path = ~/PS04_Problem3_SAS/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data out_lib.positive_electricity_logcost;
	set in_lib.recs2020_public_v5;
	where dollarel > 0;
	log_dollarel = log(dollarel);
run;

proc univariate data=out_lib.positive_electricity_logcost;
    var log_dollarel;
    histogram log_dollarel;
run;
```

### d.
[d-results](https://github.com/mab0y/Stats-506/blob/main/PS-04/d-results.html) 
```sas
%let in_path = ~/PS04_Problem3_SAS/input_data;
%let out_path = ~/PS04_Problem3_SAS/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

proc reg data=out_lib.positive_electricity_logcost;
    model log_dollarel=totrooms prkgplc1;
    weight nweight;
    where prkgplc1 in (0, 1);
    output out=out_lib.predicted_logcost predicted=predicted_log_dollarel;
run;
quit;
```

### e.
[e-results](https://github.com/mab0y/Stats-506/blob/main/PS-04/e-results.html) 
```sas
%let in_path = ~/PS04_Problem3_SAS/input_data;
%let out_path = ~/PS04_Problem3_SAS/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data actual_predicted;
	set out_lib.predicted_logcost(keep=predicted_log_dollarel dollarel);
    predicted_dollarel = exp(predicted_log_dollarel);
    drop predicted_log_dollarel;
run;

proc sgplot data=actual_predicted;
   scatter y=predicted_dollarel x=dollarel;
run;
```

## Problem 4 - Multiple tools
### a.
The Codebook was generated using the `codebook *` command in Stata, which provides a comprehensive overview of all variables in the dataset in a single run.

### b.
```sas
%let in_path = ~/PS04_Problem3_SAS/input_data;
%let out_path = ~/PS04_Problem3_SAS/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

proc sql;
	create table out_lib.public2022 as
	select CaseID, B3, ND2, B7_b, GH1, ppeducat, race_5cat, weight_pop
	from in_lib.public2022;

quit;
run;

proc export
    data=out_lib.public2022
    outfile="&out_path./public2022.dta"
    dbms=dta replace;
run;
```

### c.
```stata
use "C:\Users\mab0y\Downloads\public2022.dta"
```

### d.
```stata
. display _N
11667
```

### e.
```stata
. gen B3_binary=(B3>=3)

. B3_binary
```

### f.
```stata
. svyset CaseID [pw=weight_pop]

Sampling weights: weight_pop
             VCE: linearized
     Single unit: missing
        Strata 1: <one>
 Sampling unit 1: CaseID
           FPC 1: <zero>

. svy:logit c.B3_binary i.ND2 i.B7_b i.GH1 i.ppeducat i.race_5cat
(running logit on estimation sample)

Survey: Logistic regression

Number of strata =      1                        Number of obs   =      11,667
Number of PSUs   = 11,667                        Population size = 255,114,223
                                                 Design df       =      11,666
                                                 F(17, 11650)    =       56.70
                                                 Prob > F        =      0.0000

------------------------------------------------------------------------------
             |             Linearized
   B3_binary | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         ND2 |
          2  |   .0816722   .0925755     0.88   0.378    -.0997913    .2631356
          3  |   .0618535   .0854686     0.72   0.469    -.1056792    .2293863
          4  |   .2533887   .2045978     1.24   0.216    -.1476572    .6544347
          5  |    .229354   .1672799     1.37   0.170    -.0985426    .5572505
             |
        B7_b |
          2  |   1.110649   .0488662    22.73   0.000     1.014863    1.206435
          3  |   1.806251   .0796863    22.67   0.000     1.650052    1.962449
          4  |   2.485125   .3463415     7.18   0.000     1.806238    3.164013
             |
         GH1 |
          2  |  -.0702921    .056382    -1.25   0.213    -.1808102     .040226
          3  |   .0190607   .0587346     0.32   0.746    -.0960689    .1341904
          4  |   .3465325   .0994184     3.49   0.000     .1516557    .5414092
             |
    ppeducat |
          2  |   .0767668   .1036364     0.74   0.459    -.1263778    .2799115
          3  |   .1075004   .1008067     1.07   0.286    -.0900975    .3050983
          4  |   .2288346    .099574     2.30   0.022     .0336528    .4240164
             |
   race_5cat |
          2  |   .7060141   .0810818     8.71   0.000     .5470803     .864948
          3  |   .1635498   .0711263     2.30   0.021     .0241303    .3029693
          4  |   .4567994   .1259942     3.63   0.000     .2098298    .7037691
          5  |  -.0210142   .1659436    -0.13   0.899    -.3462915    .3042631
             |
       _cons |  -.4852955   .1301287    -3.73   0.000    -.7403696   -.2302214
------------------------------------------------------------------------------
```
From the logistic regression results, the dummy variable `ND2`, which captures respondents' expectations about the chance of experiencing a natural disaster or severe weather event in the next 5 years, is not statistically significant at the 10% significance level. This suggests that, after controlling for other variables, whether the respondent’s family is better off, the same, or worse off finanicially compared to 12 month’s ago cannot be predicted by thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years.

### g.
```{r}
library(haven)
dat <- read_dta("C:/Users/mab0y/Downloads/public2022.dta")
```

### h.
Sanitize the data:
```{r}
dat$B3_binary <- as.numeric(dat$B3>=3)
dat$ND2 <- as.factor(dat$ND2)
dat$B7_b <- as.factor(dat$B7_b)
dat$GH1 <- as.factor(dat$GH1)
dat$ppeducat <- as.factor(dat$ppeducat)
dat$race_5cat <- as.factor(dat$race_5cat)
```
Conduct the regression, here I refer to the [answer](# https://stats.stackexchange.com/questions/504209/understanding-the-deviance-and-pseudo-r2-from-a-glm) to calculate pseudo-R2
```{r}
#| include: false
library(survey)
```

```{r}
#| warning: false
survey_design <- svydesign(id = ~ CaseID, weight = ~ weight_pop, data = dat)

residual_model<-svyglm(B3_binary ~ ND2 + B7_b + GH1 + ppeducat + race_5cat, design=survey_design,family = "quasibinomial")
null_model <- svyglm(B3_binary ~ 1, design = survey_design, family = binomial())

pseudo_R2 <- 1 - (deviance(residual_model) / deviance(null_model))
```

```{r}
pseudo_R2
```